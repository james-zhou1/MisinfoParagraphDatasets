{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12406\\AppData\\Local\\Temp\\ipykernel_10840\\216391889.py:7: DtypeWarning: Columns (1,8,10,11,13,14,15,17,18,27,37,38,63,65,81,114,148,158,166,167) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../datasets/3_data_paragraph_labelled_prepared.csv', encoding='utf-8', encoding_errors='replace')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df = pd.read_csv('../datasets/3_data_paragraph_labelled_prepared.csv', encoding='utf-8', encoding_errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128213"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>positive_evidence_text</th>\n",
       "      <th>positive_evidence_title</th>\n",
       "      <th>negative_evidence_text</th>\n",
       "      <th>negative_evidence_title</th>\n",
       "      <th>split</th>\n",
       "      <th>dataset</th>\n",
       "      <th>...</th>\n",
       "      <th>evidence_type</th>\n",
       "      <th>is_auth</th>\n",
       "      <th>evidence_id</th>\n",
       "      <th>evidence_label</th>\n",
       "      <th>article</th>\n",
       "      <th>evidence</th>\n",
       "      <th>entropy</th>\n",
       "      <th>Misinfo_type</th>\n",
       "      <th>speaker</th>\n",
       "      <th>veracity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake_news_corpus</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake_news_corpus</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake_news_corpus</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake_news_corpus</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake_news_corpus</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 233 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   claim       label  question  answer  positive_evidence_text  \\\n",
       "0    NaN  unreliable       NaN     NaN                     NaN   \n",
       "1    NaN        fake       NaN     NaN                     NaN   \n",
       "2    NaN  unreliable       NaN     NaN                     NaN   \n",
       "3    NaN  unreliable       NaN     NaN                     NaN   \n",
       "4    NaN   clickbait       NaN     NaN                     NaN   \n",
       "\n",
       "   positive_evidence_title  negative_evidence_text  negative_evidence_title  \\\n",
       "0                      NaN                     NaN                      NaN   \n",
       "1                      NaN                     NaN                      NaN   \n",
       "2                      NaN                     NaN                      NaN   \n",
       "3                      NaN                     NaN                      NaN   \n",
       "4                      NaN                     NaN                      NaN   \n",
       "\n",
       "  split           dataset  ... evidence_type is_auth evidence_id  \\\n",
       "0   NaN  fake_news_corpus  ...           NaN     NaN         NaN   \n",
       "1   NaN  fake_news_corpus  ...           NaN     NaN         NaN   \n",
       "2   NaN  fake_news_corpus  ...           NaN     NaN         NaN   \n",
       "3   NaN  fake_news_corpus  ...           NaN     NaN         NaN   \n",
       "4   NaN  fake_news_corpus  ...           NaN     NaN         NaN   \n",
       "\n",
       "  evidence_label article evidence entropy Misinfo_type speaker  veracity  \n",
       "0            NaN     NaN      NaN     NaN          NaN     NaN         2  \n",
       "1            NaN     NaN      NaN     NaN          NaN     NaN         2  \n",
       "2            NaN     NaN      NaN     NaN          NaN     NaN         2  \n",
       "3            NaN     NaN      NaN     NaN          NaN     NaN         2  \n",
       "4            NaN     NaN      NaN     NaN          NaN     NaN         3  \n",
       "\n",
       "[5 rows x 233 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fake_news_corpus', 'fa_kes', 'fakenewsamt', 'banfakenews', 'benjamin_political_news', 'isot_fake_news', 'ti_cnn', 'celebrity', 'ct_fan']\n"
     ]
    }
   ],
   "source": [
    "# Extract distinct categories with data\n",
    "categories_with_data = list(df['dataset'].unique())\n",
    "\n",
    "print(categories_with_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "banfakenews                58478\n",
      "isot_fake_news             44898\n",
      "ti_cnn                     20015\n",
      "ct_fan                      2462\n",
      "fa_kes                       804\n",
      "celebrity                    500\n",
      "fakenewsamt                  480\n",
      "benjamin_political_news      326\n",
      "fake_news_corpus             250\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print number of entries for each dataset\n",
    "print(df['dataset'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for fake_news_corpus:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00        51\n",
      "\n",
      "    accuracy                           1.00        51\n",
      "   macro avg       1.00      1.00      1.00        51\n",
      "weighted avg       1.00      1.00      1.00        51\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classification Report for fa_kes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.51      0.55       115\n",
      "           2       0.45      0.52      0.48        86\n",
      "\n",
      "    accuracy                           0.52       201\n",
      "   macro avg       0.52      0.52      0.52       201\n",
      "weighted avg       0.53      0.52      0.52       201\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classification Report for fakenewsamt:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.48      0.61      0.54        57\n",
      "           2       0.45      0.32      0.38        56\n",
      "\n",
      "    accuracy                           0.47       113\n",
      "   macro avg       0.46      0.47      0.46       113\n",
      "weighted avg       0.46      0.47      0.46       113\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classification Report for banfakenews:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      1.00      0.99     13943\n",
      "           2       0.99      0.72      0.83       677\n",
      "\n",
      "    accuracy                           0.99     14620\n",
      "   macro avg       0.99      0.86      0.91     14620\n",
      "weighted avg       0.99      0.99      0.99     14620\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classification Report for benjamin_political_news:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.61      0.64        31\n",
      "           2       0.78      0.82      0.80        51\n",
      "\n",
      "    accuracy                           0.74        82\n",
      "   macro avg       0.73      0.72      0.72        82\n",
      "weighted avg       0.74      0.74      0.74        82\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classification Report for isot_fake_news:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.81      0.86      1172\n",
      "           2       0.96      0.99      0.97      5719\n",
      "\n",
      "    accuracy                           0.96      6891\n",
      "   macro avg       0.94      0.90      0.92      6891\n",
      "weighted avg       0.96      0.96      0.95      6891\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classification Report for ti_cnn:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.84      0.87      1991\n",
      "           2       0.90      0.93      0.92      3013\n",
      "\n",
      "    accuracy                           0.90      5004\n",
      "   macro avg       0.90      0.89      0.89      5004\n",
      "weighted avg       0.90      0.90      0.90      5004\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classification Report for celebrity:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.64      0.65        64\n",
      "           2       0.63      0.64      0.63        61\n",
      "\n",
      "    accuracy                           0.64       125\n",
      "   macro avg       0.64      0.64      0.64       125\n",
      "weighted avg       0.64      0.64      0.64       125\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classification Report for ct_fan:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.30      0.40       160\n",
      "           2       0.77      0.93      0.84       405\n",
      "\n",
      "    accuracy                           0.75       565\n",
      "   macro avg       0.69      0.61      0.62       565\n",
      "weighted avg       0.73      0.75      0.72       565\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Dictionary to store f1-scores and classification reports\n",
    "f1_scores = {}\n",
    "classification_reports = {}\n",
    "\n",
    "for dset in categories_with_data:\n",
    "    # Filter dataset and prepare veracity labels\n",
    "    temp_df = df[df.dataset == dset]\n",
    "    temp_df = temp_df[temp_df.veracity != 3]\n",
    "    temp_df = temp_df[temp_df.veracity.notna()]\n",
    "    temp_df['veracity'] = temp_df['veracity'].astype(int)\n",
    "    \n",
    "    # Remove rows with NaN in article_content\n",
    "    temp_df = temp_df[temp_df['article_content'].notna()]\n",
    "    \n",
    "    # Extract top 40 keywords from text\n",
    "    vectorizer = CountVectorizer(max_features=40, stop_words='english')\n",
    "    X = vectorizer.fit_transform(temp_df['article_content'])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Convert to DataFrame for easier handling\n",
    "    keyword_features = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        keyword_features, \n",
    "        temp_df.veracity.values, \n",
    "        test_size=0.25, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    clf = RandomForestClassifier(max_depth=20, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate and store f1-score\n",
    "    f1 = f1_score(y_test, preds, average='macro')\n",
    "    f1_scores[dset] = f1 * 100\n",
    "    \n",
    "    # Generate and store classification report\n",
    "    report = classification_report(y_test, preds)\n",
    "    classification_reports[dset] = report\n",
    "    print(f\"\\nClassification Report for {dset}:\")\n",
    "    print(report)\n",
    "    print(\"-\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1-Scores (Macro Average) by Dataset:\n",
      "fake_news_corpus: 100.0%\n",
      "fa_kes: 51.5%\n",
      "fakenewsamt: 45.7%\n",
      "banfakenews: 91.3%\n",
      "benjamin_political_news: 72.2%\n",
      "isot_fake_news: 91.8%\n",
      "ti_cnn: 89.1%\n",
      "celebrity: 64.0%\n",
      "ct_fan: 62.2%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nF1-Scores (Macro Average) by Dataset:\")\n",
    "for dataset, score in f1_scores.items():\n",
    "    print(f\"{dataset}: {score:.1f}%\")\n",
    "    # print(\"\\nClassification Report:\")\n",
    "    # print(classification_reports[dataset])\n",
    "    # print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "127hw4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
